{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another unsupervised learning model that is widely used in areas of temporal pattern recognition such as speech recognition, gesture recognition, and bioinformatics\n",
    "\n",
    "<img src = \"https://mioalter.files.wordpress.com/2015/12/thickerhmm4.png\"/ width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps: Markov Chains ##\n",
    "- Hidden Markov Models are based on a concept called Markov Chains\n",
    "- If you've taken stat 110 or stat 171 or studied this subject before, feel free to skip this section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chains are an extremely useful concept in probability and statistics that model the idea of future outcomes being independent of the past once you know the present. For example, this is a highly unrealistic model, but suppose you live in an imaginary world where there are only two possibilities for weather on a given day: sunny and rainy, and you are trying to predict whether it will rain tomorrow. Suppose you also know the part history of the weather. So, the data you have are whether it was rainy or sunny today, whether it was rainy or sunny yesterday, etc. The **Markov property** would say that if you know the weather today, none of the past history besides today can be used predict the weather tomorrow. Stating this in terms of probability:\n",
    "\n",
    "$$P(X_{n+1} | X_1, X_2,X_3, \\dots, X_n) = P(X_{n+1} | X_n)$$\n",
    "\n",
    "If you haven't seen this notation before, don't worry. By $P(\\textbf{something})$, we mean the probability that \"something\" happens, and the vertical line is the past information that we have that we are conditioning on. For example, if X is the outcome you get from rolling a standard die, $P(X = 3 | X< 4) = \\frac{1}{3}$, since the possibilities are $1,2,$ and $3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, what are the problems with this as a model? For one thing, it's pretty unrealisic to have weather only depend on the most recent day. But, we can solve that by expanding the state space of our markov chain; for example, we could have our $X_i$'s represent the weather from the last three days. This also isn't entirely satisfying though, since a priori, we don't really know how far back in time we want our model to go. That's where **Hidden Markov Models** are useful: we assume that the variables we observe (in this case, the weather) are dependent on some *hidden* Markov chain that we don't get to observe (in this case, some weird process that goes on up in the sky that we can't see).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image included at the beginning of this preview sums up the process pretty well. The $H_i$s are the hidden Markov chain, and we don't get to observe these but we assume they exist for the purpose of our model.\n",
    "The $O_i$s are what we observe. These are called the **emissions** of the model. First, notice that without knowing the $H_i$s, we can't really draw any conclusions about the independence structure of the $O_i$s. That is, the $O_i$s don't have the Markov property that we talked about above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about how this would apply to a speech recognition problem. In speech recognition, our observations are a sequence of frequencies, detected by a machine, and we are trying to deduce the sequence of phonemes that correspond to those frequencies. So, we can think of the hidden states as the true phonemes, the $H_i$s, and the frequency of sound that we observe as the $O_i$. In this particular problem, our question of interest is: How do we learn what the $H_i$s are? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Parameters of HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Problems in HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
